import json
import csv
import re
import sys
import unidecode
import smartypants
import spacy
from spacy.util import compile_infix_regex, compile_prefix_regex, compile_suffix_regex, filter_spans
from spacy.tokens import Doc, Span
from spacy.matcher import PhraseMatcher
from bs4 import BeautifulSoup
import eng_to_ipa as ipa
import subprocess


class LatinToShavian:
    """A class for converting Latin text to Shavian script."""
    
    def __init__(self, readlex_path="readlex/readlex_converter.json", phrases_path="readlex/readlex_converter_phrases.json"):
        """Initialize the converter with dictionaries and spaCy model."""
        self.readlex_path = readlex_path
        self.phrases_path = phrases_path
        
        # Load ReadLex dictionary
        with open(self.readlex_path, 'r', encoding="utf-8") as file:
            json_data = file.read()
        self.readlex_dict: dict[str, list[dict[str, str]]] = json.loads(json_data)

        # In-memory cache for IPA conversions
        self.ipa_cache = {}

        # Categories of letters that determine how a following 's is pronounced
        self.s_follows: set[str] = {"ð‘", "ð‘‘", "ð‘’", "ð‘“", "ð‘”"}
        self.uhz_follows: set[str] = {"ð‘•", "ð‘–", "ð‘—", "ð‘Ÿ", "ð‘ ", "ð‘¡"}
        self.z_follows: set[str] = {"ð‘š", "ð‘›", "ð‘œ", "ð‘", "ð‘ž", "ð‘™", "ð‘¤", "ð‘¥", "ð‘¯", "ð‘¸", "ð‘¹", "ð‘º", "ð‘»", "ð‘¼", "ð‘½"}
        self.consonants = set.union(self.s_follows, self.uhz_follows, self.z_follows)

        # Contractions that need special treatment since the separate words are not as they appear in the dictionary
        self.contraction_start: dict[str, str] = {
            "ai": "ð‘±", "ca": "ð‘’ð‘­", "do": "ð‘›ð‘´", "does": "ð‘›ð‘³ð‘Ÿ", "did": "ð‘›ð‘¦ð‘›", "sha": "ð‘–ð‘­",
            "wo": "ð‘¢ð‘´", "y'": "ð‘˜"
        }
        self.contraction_end: dict[str, str] = {
            "n't": "ð‘¯ð‘‘", "all": "ð‘·ð‘¤", "'ve": "ð‘", "'ll": "ð‘¤", "'m": "ð‘¥", "'d": "ð‘›", "'re": "ð‘¼"
        }

        # Common prefixes and suffixes used in new coinings
        self.prefixes: dict[str, str] = {
            "anti": "ð‘¨ð‘¯ð‘‘ð‘¦", "counter": "ð‘’ð‘¬ð‘¯ð‘‘ð‘¼", "de": "ð‘›ð‘°", "dis": "ð‘›ð‘¦ð‘•",
            "esque": "ð‘§ð‘•ð‘’", "hyper": "ð‘£ð‘²ð‘ð‘¼", "hypo": "ð‘£ð‘²ð‘ð‘´", "mega": "ð‘¥ð‘§ð‘œð‘©",
            "meta": "ð‘¥ï¿½ï¿½ð‘‘ð‘©", "micro": "ð‘¥ð‘§ð‘’ð‘®ð‘´", "multi": "ð‘³ð‘¤ð‘‘ð‘¦", "mis": "ð‘¥ð‘¦ð‘•",
            "neuro": "ð‘¯ð‘˜ð‘«ð‘¼ð‘´", "non": "ð‘¯ð‘ªð‘¯", "o'er": "ð‘´ð‘¼", "out": "ð‘¬ð‘‘", "over": "ð‘´ð‘ð‘¼",
            "poly": "ð‘ð‘ªð‘¤ð‘¦", "post": "ð‘ð‘´ð‘•ð‘‘", "pre": "ð‘ð‘®ð‘°", "pro": "ð‘ð‘®ð‘´",
            "pseudo": "ð‘•ð‘¿ð‘›ð‘´", "re": "ð‘®ð‘°", "sub": "ð‘•ð‘³ð‘š", "super": "ð‘•ð‘µð‘ð‘¼",
            "ultra": "ð‘³ð‘¤ð‘‘ð‘®ð‘©", "un": "ð‘³ï¿½ï¿½", "under": "ð‘³ð‘¯ð‘›ð‘¼"
        }
        self.suffixes: dict[str, str] = {
            "able": "ð‘©ð‘šð‘©ð‘¤", "bound": "ð‘šð‘¬ð‘¯ð‘›", "ful": "ð‘“ð‘©ð‘¤", "hood": "ð‘£ð‘«ð‘›",
            "ish": "ð‘¦ð‘–", "ism": "ð‘¦ð‘Ÿð‘©ð‘¥", "less": "ð‘¤ð‘©ð‘•", "like": "ð‘¤ð‘²ð‘’", "ness": "ð‘¯ð‘©ð‘•"
        }
        self.affixes: dict[str, str] = self.prefixes | self.suffixes

        # Words that sometimes change spelling before 'to'
        self.have_to: dict[str, str] = {"have": "ð‘£ð‘¨ð‘“", "has": "ð‘£ð‘¨ð‘•"}
        self.vbd_to: dict[str, str] = {"used": "ð‘¿ð‘•ð‘‘", "unused": "ð‘³ð‘¯ð‘¿ð‘•ð‘‘", "supposed": "ð‘•ð‘©ð‘ð‘´ð‘•ð‘‘"}
        self.before_to: dict[str, str] = self.have_to | self.vbd_to

        # Suffixes that follow numerals in ordinal numbers
        self.ordinal_suffixes: dict[str, str] = {"st": "ð‘•ð‘‘", "nd": "ð‘¯ð‘›", "rd": "ð‘®ð‘›", "th": "ð‘”", "s": "ð‘Ÿ"}

        # Entity types that get namer dots
        self.namer_dot_ents: set[str] = {"PERSON", "FAC", "ORG", "GPE", "LOC", "PRODUCT", "EVENT", "WORK_OF_ART", "LAW"}

        # Initialize phonetic mapping
        self._initialize_phonetic_mapping()
        
        # Initialize spaCy
        self._initialize_spacy()
        
    def _initialize_phonetic_mapping(self):
        """Initialize IPA to Shavian phonetic mapping."""
        # IPA to Shavian mapping based on standard English pronunciation
        self.ipa_to_shavian = {
            # Vowels
            'i': 'ð‘¦', 'Éª': 'ð‘¦', 'iË': 'ð‘°', 'e': 'ð‘§', 'eÉª': 'ð‘±', 'É›': 'ð‘§', 'Ã¦': 'ð‘¨',
            'É‘': 'ð‘­', 'É‘Ë': 'ð‘­', 'É’': 'ð‘ª', 'É”': 'ð‘ª', 'É”Ë': 'ð‘·', 'oÊŠ': 'ð‘´', 'ÊŠ': 'ð‘«', 'u': 'ð‘µ', 'uË': 'ð‘µ',
            'ÊŒ': 'ð‘³', 'ÉœË': 'ð‘»', 'Éœ': 'ð‘»', 'É™': 'ð‘©', 'Éš': 'ð‘¼', 'aÉª': 'ð‘²', 'aÊŠ': 'ð‘¬', 'É”Éª': 'ð‘¶',
            'ÉªÉ™': 'ð‘½', 'eÉ™': 'ð‘º', 'ÊŠÉ™': 'ð‘»',
            'a': 'ð‘¨', 'o': 'ð‘ª',  # Additional vowel mappings
            
            # Consonants
            'p': 'ð‘', 'b': 'ð‘š', 't': 'ð‘‘', 'd': 'ð‘›', 'k': 'ð‘’', 'g': 'ð‘œ', 'f': 'ð‘“',
            'v': 'ð‘', 'Î¸': 'ð‘”', 'Ã°': 'ð‘ž', 's': 'ð‘•', 'z': 'ð‘Ÿ', 'Êƒ': 'ð‘–', 'Ê’': 'ð‘ ',
            'tÊƒ': 'ð‘—', 'dÊ’': 'ð‘¡', 'm': 'ð‘¥', 'n': 'ð‘¯', 'Å‹': 'ð‘™', 'l': 'ð‘¤', 'L': 'ð‘¤', 'r': 'ð‘®',
            'w': 'ð‘¢', 'j': 'ð‘˜', 'h': 'ð‘£',
            
            # Additional mappings for common variations
            'x': 'ð‘’',  # Scottish 'ch' as in 'loch'
            'Ê”': '',    # Glottal stop (often silent)
            'y': 'ð‘˜',  # Consonant 'y' as in 'yes'
            '*': '',    # eng_to_ipa failure marker
            'q': 'ð‘’',  # 'q' as in 'queen'
            'c': 'ð‘’',  # 'c' as in 'cat'
            'Ê¤': 'ð‘¡',  # 'j' as in 'jam'
            'Ê§': 'ð‘—',  # 'ch' as in 'chair'
            'Ê“': 'ð‘ ',  # 'zh' as in 'vision'
            ':': '',    # Length marker (remove)
        }
        
        # Common English letter combinations to IPA patterns
        self.letter_to_ipa_patterns = {
            # Vowel patterns (simplified - will be handled contextually)
            'a': ['Ã¦'],      # Default to 'cat' sound
            'e': ['e'],      # Default to 'bed' sound  
            'i': ['Éª'],      # Default to 'bit' sound
            'o': ['É’'],      # Default to 'lot' sound
            'u': ['ÊŠ'],      # Default to 'put' sound
            'y': ['j'],      # Default to consonant 'y'
            
            # Consonant patterns
            'b': ['b'],
            'd': ['d'],
            'f': ['f'],
            'h': ['h'],
            'j': ['dÊ’'],
            'k': ['k'],
            'l': ['l'],
            'm': ['m'],
            'n': ['n'],
            'p': ['p'],
            'q': ['k'],
            'r': ['r'],
            't': ['t'],
            'v': ['v'],
            'w': ['w'],
            'x': ['ks'],
            'z': ['z'],
        }
        
    def _clean_ipa(self, ipa_str: str) -> str:
        """Remove non-Shavian IPA characters like stress marks."""
        # Remove stress marks and other IPA symbols that don't have Shavian equivalents
        cleaned = re.sub(r'[ËˆËŒËˆËŒ]', '', ipa_str)  # Remove stress marks
        cleaned = re.sub(r'[Ë]', '', cleaned)      # Remove length marks (handled contextually)
        return cleaned

    def _espeak_to_ipa(self, word: str) -> str:
        """Convert word to IPA using espeak."""
        try:
            result = subprocess.run(['espeak', '-q', '-x', word], 
                                  capture_output=True, text=True, check=True)
            espeak_ipa = result.stdout.strip()
            return self._convert_espeak_to_standard_ipa(espeak_ipa)
        except (subprocess.CalledProcessError, FileNotFoundError):
            return word  # Return original word if espeak fails
    
    def _convert_espeak_to_standard_ipa(self, espeak_ipa: str) -> str:
        """Convert espeak IPA format to standard IPA."""
        # espeak uses different symbols, convert them to standard IPA
        conversion_map = {
            'A': 'É‘', 'a': 'Ã¦', 'E': 'É›', 'e': 'e', 'I': 'Éª', 'i': 'i',
            'O': 'É”', 'o': 'o', 'U': 'ÊŠ', 'u': 'u', '@': 'É™', '3': 'Éœ',
            'V': 'ÊŒ', 'N': 'Å‹', 'S': 'Êƒ', 'Z': 'Ê’', 'T': 'Î¸', 'D': 'Ã°',
            'tS': 'tÊƒ', 'dZ': 'dÊ’', 'j': 'j', 'w': 'w', 'h': 'h',
            'p': 'p', 'b': 'b', 't': 't', 'd': 'd', 'k': 'k', 'g': 'g',
            'f': 'f', 'v': 'v', 's': 's', 'z': 'z', 'm': 'm', 'n': 'n',
            'l': 'l', 'r': 'r'
        }
        
        # Handle stress marks
        espeak_ipa = re.sub(r"'", '', espeak_ipa)  # Remove primary stress
        espeak_ipa = re.sub(r',', '', espeak_ipa)  # Remove secondary stress
        
        # Convert characters
        result = ""
        i = 0
        while i < len(espeak_ipa):
            # Try digraphs first
            if i < len(espeak_ipa) - 1:
                digraph = espeak_ipa[i:i+2]
                if digraph in conversion_map:
                    result += conversion_map[digraph]
                    i += 2
                    continue
            
            # Try single characters
            char = espeak_ipa[i]
            if char in conversion_map:
                result += conversion_map[char]
            else:
                result += char  # Keep unknown characters
            
            i += 1
        
        return result

    def _get_ipa_from_text(self, word: str) -> str:
        """Convert English text to IPA using eng_to_ipa, fallback to espeak if needed."""
        if word in self.ipa_cache:
            return self.ipa_cache[word]

        ipa_str = ipa.convert(word)
        # If eng_to_ipa returns the word with a '*' suffix, it means it couldn't convert it
        # In this case, try espeak as a fallback
        if ipa_str.endswith('*'):
            espeak_result = self._espeak_to_ipa(word)
            if espeak_result != word:  # espeak succeeded
                self.ipa_cache[word] = self._clean_ipa(espeak_result)
                return self.ipa_cache[word]
            else:  # espeak also failed, return original word
                self.ipa_cache[word] = word
                return self.ipa_cache[word]
        self.ipa_cache[word] = self._clean_ipa(ipa_str)
        return self.ipa_cache[word]
        
    
    def _ipa_to_shavian(self, ipa: str) -> str:
        """Convert IPA to Shavian script."""
        shavian = ""
        i = 0
        
        while i < len(ipa):
            # Try digraphs first
            if i < len(ipa) - 1:
                digraph = ipa[i:i+2]
                if digraph in self.ipa_to_shavian:
                    shavian += self.ipa_to_shavian[digraph]
                    i += 2
                    continue
            
            # Try single characters
            char = ipa[i]
            if char in self.ipa_to_shavian:
                shavian += self.ipa_to_shavian[char]
            else:
                # Keep unknown IPA symbols as-is
                shavian += char
            
            i += 1
        
        return shavian
    
    def _phonetic_transliterate(self, word: str) -> str:
        """Convert a word to Shavian using phonetic rules."""
        # Convert to IPA first
        ipa = self._get_ipa_from_text(word)
        
        # If the IPA is the same as the original word, eng_to_ipa couldn't convert it
        # Return the original word unchanged
        if ipa == word:
            return word
        
        # Convert IPA to Shavian
        shavian = self._ipa_to_shavian(ipa)
        
        # Add dot for proper nouns (words starting with uppercase)
        prefix = "Â·" if word[0].isupper() else ""
        
        # Add phonetic warning symbol
        return prefix + shavian + "[p]"

    def _initialize_spacy(self):
        """Initialize spaCy model and custom tokenizer."""
        # Load spaCy, excluding pipeline components that are not required
        self.nlp = spacy.load("en_core_web_sm", exclude=["parser", "lemmatizer", "textcat"])

        # Customise the spaCy tokeniser to ensure that initial and final dashes and dashes between words aren't stuck to one
        # of the surrounding words
        # Prefixes
        spacy_prefixes: list[str] = self.nlp.Defaults.prefixes + [r'''^[-â€“â€”]+''',]
        prefix_regex = compile_prefix_regex(spacy_prefixes)
        self.nlp.tokenizer.prefix_search = prefix_regex.search
        # Infixes
        spacy_infixes: list[str] = self.nlp.Defaults.infixes + [r'''[.,?!:;\-â€“â€”"~\(\)\[\]]+''',]
        infix_regex = compile_infix_regex(spacy_infixes)
        self.nlp.tokenizer.infix_finditer = infix_regex.finditer
        # Suffixes
        spacy_suffixes: list[str] = self.nlp.Defaults.suffixes + [r'''[-â€“â€”]+$''',]
        suffix_regex = compile_suffix_regex(spacy_suffixes)
        self.nlp.tokenizer.suffix_search = suffix_regex.search

        # Initialize phrase matcher
        self._initialize_phrase_matcher()
        
    def _initialize_phrase_matcher(self):
        """Initialize the phrase matcher with phrases from the phrases file."""
        def add_span(matcher, doc, i, matches):
            match_id, start, end = matches[i]

        # Define the phrase to match
        with open(self.phrases_path, "r", newline="") as f:
            reader = csv.reader(f)
            phrases = [row[0] for row in reader if row]
        phrase_patterns: list[Doc] = [self.nlp.make_doc(phrase) for phrase in phrases]
        self.phrase_matcher = PhraseMatcher(self.nlp.vocab, attr="LOWER")
        self.phrase_matcher.add("phrases", phrase_patterns, on_match=add_span)

    def tokenise(self, text: str) -> spacy.tokens.Doc:
        """Tokenise and tag the text using spaCy as doc."""
        doc = self.nlp(text)
        phrase_matches = self.phrase_matcher(doc)
        phrase_spans: list[Span] = []
        for match_id, start, end in phrase_matches:
            span = Span(doc, start, end, label=match_id)
            phrase_spans.append(span)

        filtered_spans = filter_spans(phrase_spans)

        with doc.retokenize() as retokenizer:
            for span in filtered_spans:
                retokenizer.merge(span)

        # Expand person entities to include titles and take initial 'the' out of entity names
        titles: set[str] = {
            "archbishop", "archdeacon", "baron", "baroness", "bishop", "captain", "count", "countess",
            "cpt", "dame", "deacon", "doctor", "dr.", "dr", "duchess", "duke", "earl", "emperor",
            "empress", "gov.", "gov", "governor", "justice", "king", "lady", "lord", "marchioness",
            "marquess", "marquis", "miss", "missus", "mister", "mistress", "mr.", "mr", "mrs.", "mrs",
            "ms.", "ms", "mx.", "mx", "pope", "pres.", "pres", "president", "prince", "princess",
            "prof.", "prof", "professor", "queen", "rev.", "rev", "reverend", "saint", "sen.", "sen",
            "senator", "sir", "st.", "st", "viscount", "viscountess"
        }
        new_ents: list[Span] = []
        for ent in doc.ents:
            # Only check for title if it's a person and not the first token
            if ent.label_ == "PERSON" and ent.start != 0:
                prev_token = doc[ent.start - 1]
                if prev_token.lower_ in titles:
                    new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)
                    new_ents.append(new_ent)
                else:
                    new_ents.append(ent)
            elif ent.label_ in self.namer_dot_ents:
                if doc[ent.start].lower_ == "the":
                    new_ent = Span(doc, ent.start + 1, ent.end, label=ent.label)
                    new_ents.append(new_ent)
                else:
                    new_ents.append(ent)
            else:
                new_ents.append(ent)

        filtered_ents = filter_spans(new_ents)
        doc.ents = tuple(filtered_ents)

        return doc

    def convert(self, doc: spacy.tokens.Doc) -> str:
        """Apply a series of tests to each token to determine how to Shavianise it."""
        text_split_shaw: str = ""

        for token in doc:
            # Leave HTML tags unchanged
            if token.tag_ == "HTML":
                text_split_shaw += token.text

            # Convert contractions
            elif token.lower_ in self.contraction_start and token.i < len(doc) - 1 and doc[token.i + 1].lower_ in self.contraction_end:
                text_split_shaw += self.contraction_start[token.lower_]
            elif token.lower_ in self.contraction_end:
                prefix: str = "ð‘©" if token.lower_ != "ð‘¼" and text_split_shaw and text_split_shaw[-1] in self.consonants else ""
                text_split_shaw += prefix + self.contraction_end[token.lower_] + token.whitespace_

            # Convert possessive 's
            elif token.lower_ == "'s":
                suffix: str = "ð‘•" if text_split_shaw[-1] in self.s_follows else "ð‘©ð‘Ÿ" if text_split_shaw[-1] in self.uhz_follows else "ð‘Ÿ"
                text_split_shaw += suffix + token.whitespace_

            # Convert possessive '
            elif token.lower_ == "'" and token.tag_ == "POS":
                text_split_shaw += token.whitespace_

            # Convert verbs that change pronunciation before 'to', e.g. 'have to', 'used to', 'supposed to'
            elif token.lower_ in self.before_to and token.i < len(doc) - 1 and doc[token.i + 1].lower_ == "to":
                # 'have' only changes pronunciation where 'have to' means 'must'
                if token.lower_ in self.have_to and doc[token.i + 2].tag_ in ["VB", "VBP"]:
                    text_split_shaw += self.have_to[token.lower_] + token.whitespace_
                # 'used', 'supposed' etc. only change pronunciation in the past tense, not past participle
                elif token.lower_ in self.vbd_to and token.tag_ in ["VBD", "VBN", "."]:
                    text_split_shaw += self.vbd_to[token.lower_] + token.whitespace_

            # Match ordinal numbers represented by a numeral and a suffix
            elif re.fullmatch(r"([0-9]+(?:[, .]?[0-9]+)*)(st|nd|rd|th|s)", token.lower_):
                number, number_suffix = re.match(r"([0-9]+(?:[, .]?[0-9]+)*)(st|nd|rd|th|s)", token.lower_).groups()
                text_split_shaw += number + self.ordinal_suffixes[number_suffix] + token.whitespace_

            # Loop through the words in the ReadLex and look for matches, and only apply the namer dot to the first word
            # in a name (or not at all for initialisms marked with â¸°)
            elif token.lower_ in self.readlex_dict:
                for i in self.readlex_dict.get(token.lower_, []):
                    # Match the part of speech for heteronyms
                    if i["tag"] == token.tag_:
                        prefix: str = "Â·" if token.ent_iob_ == "B" and token.ent_type_ in self.namer_dot_ents and not i["Shaw"].startswith("â¸°") else ""
                        text_split_shaw += prefix + i["Shaw"] + token.whitespace_
                        break

                    # For any proper nouns not in the ReadLex, match if an identical common noun exists
                    elif (i["tag"] in ["NN", "0"] and token.tag_ == "NNP") or (i["tag"] in ["NNS", "0"] and token.tag_ == "NNPS"):
                        prefix = "Â·" if token.ent_iob_ == "B" and token.ent_type_ in self.namer_dot_ents and not i["Shaw"].startswith("â¸°") else ""
                        text_split_shaw += prefix + i["Shaw"] + token.whitespace_
                        break

                    # Match words with only one pronunciation
                    elif i["tag"] == "0":
                        prefix = "Â·" if token.ent_iob_ == "B" and token.ent_type_ in self.namer_dot_ents and not i["Shaw"].startswith("â¸°") else ""
                        text_split_shaw += prefix + i["Shaw"] + token.whitespace_
                        break

            # Apply additional tests where there is still no match
            else:
                found: bool = False
                constructed_warning: str = "[c]"
                '''
                Try to construct a match using common prefixes and suffixes and include a warning symbol to aid proof
                reading
                '''
                for j in self.affixes:
                    if token.lower_.startswith(j) and j in self.prefixes:
                        prefix: str = self.prefixes[j]
                        suffix: str = ""
                        target_word: str = token.lower_[len(j):]
                    elif token.lower_.endswith(j) and j in self.suffixes:
                        prefix = ""
                        suffix = self.suffixes[j]
                        target_word = token.lower_[:-len(j)]
                    else:
                        continue
                    if target_word in self.readlex_dict:
                        found = True
                        for i in self.readlex_dict.get(target_word):
                            prefix = "Â·" if token.ent_iob_ == "B" and token.ent_type_ in self.namer_dot_ents and not i["Shaw"].startswith("â¸°") else prefix
                            text_split_shaw += prefix + i["Shaw"] + suffix + constructed_warning + token.whitespace_
                            break

                # Try to construct plurals if not expressly included in the ReadLex, e.g. plurals of proper names.
                if token.lower_.endswith("s"):
                    target_word = token.lower_[:-1]
                    if target_word in self.readlex_dict:
                        found = True
                        for i in self.readlex_dict.get(target_word):
                            suffix = "ð‘•" if i["Shaw"][-1] in self.s_follows else "ð‘©ð‘Ÿ" if i["Shaw"][-1] in self.uhz_follows else "ð‘Ÿ"
                            prefix = "Â·" if token.ent_iob_ == "B" and token.ent_type_ in self.namer_dot_ents and not i["Shaw"].startswith("â¸°") else ""
                            text_split_shaw += prefix + i["Shaw"] + suffix + constructed_warning + token.whitespace_
                            break

                if found is not False:
                    continue
                
                # Phonetic fallback: if no match found, try phonetic transliteration
                if token.text.isalpha():
                    phonetic_result = self._phonetic_transliterate(token.text)
                    text_split_shaw += phonetic_result + token.whitespace_
                else:
                    text_split_shaw += token.text + token.whitespace_

        return text_split_shaw

    def convert_text(self, text: str) -> str:
        """Convert Latin text to Shavian script."""
        # Create the string that will contain the Shavianised text.
        text_shaw: str = ""

        # Split up the string to reduce the risk of spaCy exceeding memory limits
        if text.strip().casefold().startswith("<!doctype html"):
            style_pattern: str = r"(<style\b[^>]*>.*?</style>)"
            script_pattern: str = r"(<script\b[^>]*>.*?</script>)"
            html_pattern: str = r"(?!(?:<style[^>]*?>.*?</style>|<script[^>]*?>.*?</script>))(<.*?>)"
            html_patterns: str = f"{style_pattern}|{script_pattern}|{html_pattern}"
            text_split: list[str] = re.split(html_patterns, text, flags=re.DOTALL)
            for text_part in text_split:
                if text_part is None:
                    pass
                elif re.fullmatch(style_pattern, text_part, flags=re.DOTALL) or re.fullmatch(
                        script_pattern, text_part, flags=re.DOTALL) or re.fullmatch(html_pattern, text_part, flags=re.DOTALL):
                    text_shaw += text_part
                else:
                    doc: spacy.tokens.Doc = self.tokenise(text_part)
                    text_shaw += self.convert(doc)

            # Convert dumb quotes, double hyphens, etc. to their typographic equivalents
            text_shaw = smartypants.smartypants(text_shaw)
            # Convert curly quotes to angle quotes
            quotation_marks: dict[str, str] = {"&#8216;": "&lsaquo;", "&#8217;": "&rsaquo;", "&#8220;": "&laquo;", "&#8221;": "&raquo;"}
            for key, value in quotation_marks.items():
                text_shaw = text_shaw.replace(key, value)

        else:
            text = unidecode.unidecode(text)
            text = re.sub(r"(\S)(\[)", r"\1 \2", text)
            text = re.sub(r"](\S)", r"] \1", text)
            text_split: list[str] = text.splitlines()
            for i in text_split:
                if len(i) < 10000:
                    doc: spacy.tokens.Doc = self.tokenise(i)
                    text_shaw += self.convert(doc) + "\n"
            # Convert dumb quotes, double hyphens, etc. to their typographic equivalents
            text_shaw = smartypants.smartypants(text_shaw)
            quotation_marks: dict[str, str] = {"&#8216;": "&lsaquo;", "&#8217;": "&rsaquo;", "&#8220;": "&laquo;", "&#8221;": "&raquo;"}
            for key, value in quotation_marks.items():
                text_shaw = text_shaw.replace(key, value)
            text_shaw = str(BeautifulSoup(text_shaw, features="html.parser"))

        return text_shaw

    def run_stdin_stdout_mode(self):
        """Run in stdin/stdout mode for long-running processes."""
        import sys
        import traceback
        
        print("READY", file=sys.stderr, flush=True)  # Signal that we're ready
        
        try:
            while True:
                try:
                    # Read a line from stdin
                    line = sys.stdin.readline()
                    if not line:  # EOF
                        break
                    
                    line = line.rstrip('\n')
                    # Parse the request format: "ID:TEXT"
                    colon_index = line.find(':')
                    if colon_index == -1:
                        continue  # Skip malformed requests
                    
                    request_id = line[:colon_index]
                    text = line[colon_index + 1:]
                    
                    # If text is empty or whitespace, just return empty string
                    if not text.strip():
                        print(f"{request_id}:", flush=True)
                        continue
                    
                    try:
                        result = self.convert_text(text)
                        print(f"{request_id}:{result}", flush=True)
                    except Exception as e:
                        print(f"Error: {e}", file=sys.stderr, flush=True)
                        print(f"{request_id}:", flush=True)
                except Exception as e:
                    print(f"Exception: {e}", file=sys.stderr, flush=True)
                    traceback.print_exc(file=sys.stderr)
        except KeyboardInterrupt:
            pass


def latin2shaw(text):
    """Legacy function for backward compatibility."""
    converter = LatinToShavian()
    try:
        result = converter.convert_text(text)
        return result
    finally:
        pass # No cache to save


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Convert Latin text to Shavian script")
    parser.add_argument("--stdin-stdout", action="store_true", 
                       help="Run in stdin/stdout mode for long-running processes")
    parser.add_argument("--text", type=str, help="Text to convert (if not using stdin/stdout mode)")
    parser.add_argument("--readlex-path", type=str, default="readlex/readlex_converter.json",
                       help="Path to ReadLex converter JSON file")
    parser.add_argument("--phrases-path", type=str, default="readlex/readlex_converter_phrases.json",
                       help="Path to phrases JSON file")
    
    args = parser.parse_args()
    
    converter = LatinToShavian(args.readlex_path, args.phrases_path)
    
    try:
        if args.stdin_stdout:
            # Run in stdin/stdout mode
            converter.run_stdin_stdout_mode()
        elif args.text:
            # Convert provided text
            result = converter.convert_text(args.text)
            print(result)
        else:
            # Read from stdin if no text provided
            text = sys.stdin.read()
            result = converter.convert_text(text)
            print(result)
    finally:
        pass # No cache to save